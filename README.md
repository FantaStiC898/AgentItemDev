# AgentItemDev: A Multimodal and Multi-agent System for Automating the Full Test Development Cycle in Medical Licensing Exams

## Overview

This project implements a pipeline using R and Python to automate the development, review, and analysis of USMLE Step 1 style test items. It leverages Large Language Models (LLMs) via the OpenAI API (or compatible endpoints) to simulate various roles in the item development lifecycle, including authors and editorial reviewers. The pipeline also includes capabilities for psychometric item analysis and optional AI-driven generation and verification of medical images associated with test items.

The core workflow involves:
1.  **Analyzing** an existing item bank against USMLE content specifications to identify areas needing new items.
2.  **Generating** new item drafts based on identified needs using an LLM agent.
3.  **Simulating** a multi-stage review and revision process involving multiple LLM agents acting as editorial staff and the original author.
4.  **Performing** classical item analysis (difficulty, discrimination, distractor analysis) on pilot test data.
5.  **(Optional)** Generating relevant medical images using either local Stable Diffusion models or online APIs (Google GenAI).
6.  **(Optional)** Verifying the generated medical images for consistency and answerability using a multimodal LLM.
7.  **Generating** comprehensive HTML reports detailing the item development lifecycle and analysis results.

## Features

*   **Item Bank Analysis:** Analyzes item bank distribution across disciplines, systems, and competencies based on defined USMLE specifications.
*   **AI-Driven Item Generation:** Creates initial item drafts (question stem, options, key, rationale) based on specific content requirements.
*   **Multi-Agent Workflow Simulation:** Mimics the authoring, editing, and revision process using distinct LLM configurations.
*   **Configurable LLM Backend:** Selects LLMs and API endpoints dynamically via a configuration file (`ModelInfo.csv`).
*   **Psychometric Analysis:** Calculates traditional item statistics (difficulty, discrimination) and performs item analysis using the `psych` and `ShinyItemAnalysis` R packages.
*   **AI-Enhanced Analysis Summary:** Uses an LLM to provide a narrative summary of item analysis results.
*   **Medical Image Generation (Optional):**
    *   **Local generation** using pre-trained diffusion models for specific medical image types:
           - Chest CT
           - Chest X-Ray (CXR)
           - Fundus images
    *   **API-based generation** using Google's Generative AI API
*   **Medical Image Verification (Optional):** Employs a multimodal LLM to assess image-text consistency and whether the question is answerable from the image.
*   **HTML Reporting:** Generates detailed, interactive HTML reports for both the item development process and item analysis results, including thinking processes, JSON data, and markdown explanations from LLMs.
*   **Language Customization:** Supports setting the interaction language for LLM prompts via `language_setting.txt`.
*   **Retry Logic:** Implements retry mechanisms for LLM API calls to handle transient errors or insufficient responses.

## Workflow

1. **Pool Analysis**: Identify areas in the item bank needing more items
2. **Item Writing**: Generate initial test items with AI
3. **Editorial Review**: Multiple AI reviewers provide feedback
4. **Item Revision**: Author revises based on feedback
5. **Secondary Review**: Editorial staff reviews revised item
6. **Final Writing**: Author creates final version
7. **Image Generation**: If needed, create appropriate medical images
8. **Image Verification**: Ensure image quality and relevance
9. **Item Analysis**: Statistical evaluation of item performance

The system primarily operates through three main R scripts:

1.  **`poolAnalysis.R`**:
    *   Reads an existing item bank (e.g., `item_bank.json`).
    *   Compares item distribution against predefined USMLE specifications (hardcoded in the script).
    *   Identifies combinations of discipline, system, and competency that are underrepresented (`areasNeedItems`).
    *   Saves detailed statistics to `currentItembankInfo.json`.
    *   Calls the `Agent_poolAnalysis.py` script (via `agentExecute` in `global.R`) to get an LLM-generated summary of the bank status and needs.
    *   Generates an HTML/PDF report (`item_bank_analysis.html`/`.pdf`) summarizing the analysis using `rmarkdown`.

2.  **`itemDevelopment.R`**:
    *   Takes an item requirement (discipline, system, competency) usually selected from the `areasNeedItems` generated by `poolAnalysis.R`.
    *   **Step 1 (Writing):** Calls `Agent_itemWriting_image.py` (or `Agent_itemWriting.py` if no image needed) to generate the initial item draft.
    *   **Step 2 (Editing):** Calls multiple instances of editing agents (`Agent_itemEditing_ReviewerX.py`) to simulate reviews from different editorial staff.
    *   **Step 3 (Revising):** Consolidates reviewer comments and calls `Agent_itemRevising.py` to simulate the author revising the item based on feedback.
    *   **Step 4 (Re-editing):** Calls `Agent_itemReediting.py` for a final editorial review, checking if the item is ready or needs more work.
    *   **Step 5 (Final Writing - Conditional):** If Step 4 indicates further revisions are needed, it calls `Agent_itemFinalwriting.py` for the author's final adjustments.
    *   **Image Generation/Verification (Conditional):** If the final item requires an image (`image_needed=True`):
        *   Calls `Agent_image_gen.py` (either `generate_medical_image` for local or `generate_medical_image_with_api` for online) to create the image.
        *   Calls `Agent_image_verification.py` (`verify_medical_image`) to assess the generated image's quality and relevance.
    *   **Reporting:** Uses `Converter.py` (called via `reticulate`) to format the output from each step into HTML segments. Combines these segments, potentially including the generated image and verification results, into a final comprehensive HTML report (`Item_Development_for_<ItemID>.html`).
    *   **History Tracking:** Appends the output and metadata of each step to a dedicated item history file (`<ItemID>.txt`).
    *   **Cleanup:** Deletes temporary files (e.g., `temp_*.txt`).

3.  **`itemAnalysis.R`**:
    *   Reads pilot item responses (`pilotResponse.csv`) and the answer key (`pilotKey.csv`).
    *   Identifies specific pilot items based on `pilotItemID`.
    *   Calculates traditional item statistics (Difficulty, Mean, SD, RIR, RIT) using `ItemAnalysis` from `ShinyItemAnalysis`.
    *   Performs distractor analysis using `DistractorAnalysis` from `ShinyItemAnalysis`.
    *   Saves the raw analysis results to `temp_itemAnalysis.txt`.
    *   Calls `Agent_itemAnalysis.py` to get an LLM-generated interpretation of the analysis results.
    *   Uses `Converter.py` to create a final HTML report (`item_Analysis_report.html`) presenting the AI summary and potentially the raw statistics.

**Helper/Core Components:**

*   **`global.R`**: Contains essential helper functions used across the R scripts (e.g., `agentExecute` for running Python agents, `replace_api_info` for setting API keys, JSON parsing, path manipulation, unique ID generation).
*   **`Agent_*.py` Scripts**: Python scripts that interact with the LLM API. They typically read fixed prompts (`*_standard.txt`, `start_writing.txt`, etc.), specific task inputs (`temp_*.txt`), and language settings (`language_setting.txt`), then call the OpenAI API and print the response.
*   **`Converter.py`**: Python script with functions to parse potentially complex LLM outputs (containing thinking steps, JSON, and markdown) and render them into structured, styled HTML.
*   **`ModelInfo.csv`**: Configuration file storing API credentials and model details for different LLMs, including specialized models for image tasks.
*   **Prompt `.txt` Files**: Contain reusable system prompts or instructions for the LLM agents (e.g., `edit_standard.txt`, `revise_initial.txt`).
*   **Temporary `.txt` Files**: Used to pass data between R and Python scripts (e.g., `temp_itemNeeded.txt`, `temp_commentNeeded.txt`).

## Prerequisites

*   **R:** Version 4.0 or later recommended.
*   **RStudio:** Recommended IDE for R development.
*   **Python:** Version 3.8 or later recommended.
*   **Conda:** For managing the Python environment.
*   **R Packages:**
    *   `jsonlite`
    *   `reticulate`
    *   `digest`
    *   `stringr`
    *   `markdown`
    *   `psych`
    *   `ShinyItemAnalysis`
    *   `rmarkdown`
    *   `base64enc` (used implicitly by reporting/image embedding)
*   **Python Packages:**
    *   `openai`
    *   `pandas`
    *   `markdown`
    *   `torch`
    *   `diffusers`
    *   `transformers`
    *   `accelerate`
    *   `Pillow` (PIL)
    *   `google-generai` (if using Google API for image generation)
*   **API Access:**
    *   Valid API key(s) and base URL(s) for OpenAI or compatible LLM providers.
    *   Valid API key for Google Generative AI (if using `generate_medical_image_with_api`).
*   **Local Models (Optional):**
    *   Pre-trained Stable Diffusion base model (the specific model used in AgentItemDev is available via [MINIM](https://github.com/WithStomach/MINIM)).
    *   Fine-tuned UNet models for specific medical image types (ChestCT, CXR, fundus) if using `generate_medical_image`.
*   **Hardware:**
    *   A modern computer capable of running R and Python.
    *   GPU (NVIDIA recommended) with sufficient VRAM is highly recommended for local image generation (`generate_medical_image`) and potentially for image verification (`verify_medical_image`).
    *   The hardware configuration used in this case study includes an Intel® Core™ i7-9750H CPU @ 2.60GHz, 64GB RAM, and an NVIDIA® GeForce® GTX 1660 Ti Laptop GPU.  

## Setup and Installation

1.  **Clone Repository:**
    ```bash
    git clone https://github.com/FantaStiC898/AgentItemDev.git
    cd AgentItemDev
    ```
2.  **Install R and RStudio:** Download and install from [CRAN](https://cran.r-project.org/) and [RStudio](https://posit.co/download/rstudio-desktop/).
3.  **Install Conda:** Download and install Miniconda or Anaconda from [conda.io](https://conda.io/projects/conda/en/latest/user-guide/install/index.html).
4.  **Create Conda Environment:**
    ```bash
    conda create -n item_env python=3.9.21
    conda activate item_env
    ```
5.  **Create `py_env.txt`:** Create a file named `py_env.txt` in the project's root directory and write the name of your conda environment into it (e.g., `item_env`). This tells `reticulate` which environment to use.
6.  **Install Python Packages:**
    ```bash
    pip install openai pandas markdown torch torchvision torchaudio # Basic torch setup
    pip install diffusers transformers accelerate Pillow google-generai
    ```
    *(Note: Ensure PyTorch is installed correctly for your system/GPU - see [pytorch.org](https://pytorch.org/))*
7.  **Install R Packages:** Open R/RStudio and run:
    ```R
    install.packages(c("jsonlite", "reticulate", "digest", "stringr", "markdown", "psych", "ShinyItemAnalysis", "rmarkdown", "base64enc"))
    ```
8.  **Configure API Keys (`ModelInfo.csv`):**
    *   Open `ModelInfo.csv`.
    *   Replace the placeholder `"xxx"` values with your actual API keys, base URLs, and desired model names for standard LLM tasks.
    *   Add rows for any specialized models (e.g., multimodal models for image verification) and set the `type` column accordingly (e.g., `image`). Ensure the model names match those expected by the API provider.
9.  **Configure Image Generation (Optional):**
    *   **Local:**
        *   Download the base Stable Diffusion model and the fine-tuned UNet models (ChestCT, CXR, fundus).
        *   Place them in accessible directories.
        *   Update the `pretrained_model_path` and paths within `model_paths` inside the `generate_medical_image` function in `Agent_image_gen.py` if they differ from the defaults.
    *   **API (Google GenAI):**
        *   Ensure the model name (`"xxx"`) in the `type="image"` column of `ModelInfo.csv` is updated to a valid Google GenAI image generation model.
        *   Run `replace_api_info('Agent_image_gen.py', ModelInfo_image)`.
10. **Prepare Prompt Files:** Review and customize the content of `.txt` files used as prompts (e.g., `edit_standard.txt`, `start_writing.txt`, `language_setting.txt`) to match your requirements and desired LLM behavior.
11. **Prepare Input Data:**
    *   Place your item bank data in `item_bank.json` format for `poolAnalysis.R`.
    *   Prepare `pilotResponse.csv` and `pilotKey.csv` for `itemAnalysis.R`. Update the `pilotItemID` vector within `itemAnalysis.R` to match the item IDs you want to analyze from these files.

## Configuration

*   **`ModelInfo.csv`:** This is the central configuration for LLMs.
    *   `api_key`: Your secret API key.
    *   `base_url`: The API endpoint URL.
    *   `model`: The specific model identifier (e.g., `gpt-4`, `gpt-3.5-turbo`).
    *   `type`: Leave blank for standard text models. Set to `image` for models intended for image generation/verification tasks handled by specific functions (like `verify_medical_image` or potentially specialized image generation APIs configured elsewhere). The `agentExecute` function in R filters models based on this type when necessary (the code will filter *out* 'image' type for most R calls, using it specifically in image-related Python calls).
*   **`py_env.txt`:** Contains the name of the Conda environment to be used by `reticulate` in R.
*   **`language_setting.txt`:** Sets the default language for LLM interactions. Modify the text within the brackets `[...]`. Can be programmatically updated using `workingLanguage()` function in `global.R`. For example, use `workingLanguage("Chinese")` sets the language to 中文. If no language is specified, AgentItemDev will default to English. **Note:** This feature's stability currently depends on the underlying LLM's instruction-following capabilities and may exhibit inconsistent behavior in some cases.
*   **Prompt `.txt` files:** Edit these files (e.g., `edit_standard.txt`, `revise_initial.txt`) to change the instructions given to the LLM agents for specific tasks.
*   **`Agent_image_gen.py`:**
    *   Modify `pretrained_model_path` and `model_paths` for local Stable Diffusion models.
    *   Set the correct Google GenAI `api_key` and `model` identifier for API-based generation.
    *   Adjust `output_dir` if needed.
*   **`itemAnalysis.R`:** Set the `pilotItemID` vector to contain the exact IDs of the items to be analyzed from the pilot data files.
*   **`itemDevelopment.R`:**
    *   Modify the selection logic for `assignment` if you don't want to process items sequentially from `areasNeedItems`.
    *   Adjust paths for image generation (`output_dir` passed to Python functions) if needed.
    *   Choose between local (`generate_medical_image`) and API (`generate_medical_image_with_api`) image generation by commenting/uncommenting the respective lines.

## Usage

Ensure your Conda environment is activated (`conda activate item_env`) before running R scripts that call Python (`reticulate`).

1.  **Run Pool Analysis:**
    *   Open R/RStudio.
    *   Ensure `item_bank.json` is present and correctly formatted.
    *   Run the `poolAnalysis.R` script.
    *   Outputs: `currentItembankInfo.json`, `item_bank_analysis.html` (or `.pdf`), console output, and the `areasNeedItems` list object in the R environment.

2.  **Run Item Development:**
    *   Open R/RStudio.
    *   Make sure `poolAnalysis.R` has been run successfully to generate `areasNeedItems`, or manually define an `assignment` data frame/list with `disciplines`, `systems`, and `competencies`.
    *   Ensure all required prompt `.txt` files and `ModelInfo.csv` are configured.
    *   Run the `itemDevelopment.R` script.
    *   Outputs:
        *   An item history file named `<ItemID>.txt`.
        *   An HTML report `Item_Development_for_<ItemID>.html`.
        *   (If image generated) An image file (e.g., `H:/medical image/CXR_local_image0_20231027_103000.png`).
        *   The new/updated item is added to the `itemDataBase` list *in the current R session*. (Note: This script does not persistently save the `itemDataBase` to a file unless added).
        *   Temporary files (`temp_*.txt`) are created during execution and deleted at the end.

3.  **Run Item Analysis:**
    *   Open R/RStudio.
    *   Ensure `pilotResponse.csv` and `pilotKey.csv` are present.
    *   Update the `pilotItemID` vector in `ItemAnalysis.R` with the IDs to analyze.
    *   Run the `ItemAnalysis.R` script.
    *   Outputs: `item_Analysis_report.html`, console output.

## File Structure
```markdown
├── AgentItemDev/ # Main project directory (example name)
│ ├── Agent_*.py # Python scripts for LLM agent tasks
│ ├── Agent_image_gen.py # Python script for image generation
│ ├── Agent_image_verification.py # Python script for image verification
│ ├── Converter.py # Python script for HTML conversion
│ ├── global.R # R script with global helper functions
│ ├── itemAnalysis.R # R script for item analysis workflow
│ ├── itemDevelopment.R # R script for item writing/review workflow
│ ├── poolAnalysis.R # R script for item bank analysis
│ ├── ModelInfo.csv # Configuration for API keys and models
│ ├── py_env.txt # Specifies the Conda environment name
│ ├── language_setting.txt # Configurable language prompt part
│ ├── *.txt # Various fixed prompt files (e.g., edit_standard.txt)
│ ├── item_bank.json # Example input item bank data
│ ├── pilotResponse.csv # Example input pilot response data
│ ├── pilotKey.csv # Example input pilot answer key data
│ ├── *.html # Output HTML reports
│ ├── *.json # Output JSON data (e.g., currentItembankInfo.json)
│ ├── *.txt # Output item history files (named by ItemID)
│ └── H:/medical image/ # Example output directory for generated images (configurable)
│ └── *.png
└── ... (potential local model directories)
```
## Detailed Function Descriptions

### `global.R`

*   `use_condaenv_from_file(file_path)`: Reads environment name from `file_path` (e.g., `py_env.txt`) and sets `reticulate` to use that Conda environment.
*   `replace_api_info(pyName, ModelInfo)`: Reads `ModelInfo.csv`, randomly selects a non-image model row (or filters based on context), and updates the `api_key`, `base_url`, and `model` placeholders in the specified Python file (`pyName`).
*   `assignTempItemID(json_object)`: Generates a unique ID for an item task based on the first letters of task specs, the current date, and a hash of the current time.
*   `itemDataBase`, `emptyItemClass`: Defines a list structure (`itemDataBase`) to hold item objects and a template (`emptyItemClass`) for a single item's data structure.
*   `safe_cat(x, append)`: A wrapper around `cat` to prevent errors when trying to print complex objects like matrices or lists directly.
*   `organizeJson(cat_output, wrongJson)`: Cleans raw LLM output (`cat_output`), attempts to extract and parse the main JSON object within it (handling surrounding text or markdown code fences). `wrongJson=T` enables more aggressive cleaning. Returns the parsed R list/object or NULL.
*   `try_functions(cat_output)`: Attempts different parsing strategies for potentially malformed JSON using `organizeJson` (specifically tries parsing nested structures). *Note: `fun2` and `fun3` are not defined in the provided snippet, suggesting this might be incomplete or simplified.*
*   `replace_file_paths_internal(py_file, script_path)`: Modifies file paths within `open()` calls in a target Python script (`py_file`) to be relative to the provided `script_path`. Ensures scripts find their auxiliary files (like prompts).
*   `agentExecute(py_file, max_retries, ModelInfo, wrongJson, script_path, requireJson)`: Orchestrates the execution of a Python agent script.
    *   Sets the R working directory.
    *   Updates file paths in the Python script using `replace_file_paths_internal`.
    *   Selects and injects API info using `replace_api_info`.
    *   Runs the Python script using `reticulate::py_run_file`.
    *   Implements a retry loop (`max_retries`).
    *   Checks response validity (length, word count).
    *   Attempts to parse JSON output using `organizeJson`.
    *   If `requireJson=TRUE`, it retries until valid JSON is obtained or `max_retries` is reached.
    *   Handles errors and provides status messages. Returns the raw result from `py_run_file` upon success.
*   `convert_to_binary(original_data, answer_key)`: Converts a data frame of participant responses (`original_data`) into a binary (0/1) format based on a provided answer key (`answer_key`).
*   `workingLanguage(new_language)`: Updates the language placeholder `[...]` in the `language_setting.txt` file with the provided `new_language` string.

### `ItemAnalysis.R`

*   *(Main Script Body)*:
    *   Loads necessary packages and sources `global.R`.
    *   Sets the Conda environment.
    *   Defines `pilotItemID` (needs user input).
    *   Reads `pilotResponse.csv` and `pilotKey.csv`.
    *   Calls `convert_to_binary` to score the data.
    *   Filters data to include only `pilotItemID` items.
    *   Calculates traditional item stats using `ShinyItemAnalysis::ItemAnalysis`.
    *   Performs distractor analysis using `ShinyItemAnalysis::DistractorAnalysis`, formatting the output table rownames.
    *   Writes raw analysis results to `temp_itemAnalysis.txt`.
    *   Calls `agentExecute` to run `Agent_itemAnalysis.py` for an AI summary.
    *   Calls `Converter.py`'s `convert_to_html` via `reticulate` to render the AI summary.
    *   Adds a title and date to the HTML content.
    *   Writes the final report to `item_Analysis_report.html` and opens it.

### `itemDevelopment.R`

*   *(Main Script Body)*: Orchestrates the multi-step item writing and review process.
    *   Loads packages, sources `global.R` and `poolAnalysis.R`.
    *   Sets language using `workingLanguage`.
    *   Selects an `assignment` (item specs) from `areasNeedItems`.
    *   Generates a `temp_itemNeededID` using `assignTempItemID`.
    *   Writes `assignment` to `temp_itemNeeded.txt`.
    *   **Step 1:** Calls `agentExecute` for `Agent_itemWriting_image.py` (or `.py`), parses JSON result (`first_attempt`).
    *   Saves raw output and metadata to `<ItemID>.txt`.
    *   Stores structured data in `itemDataBase` and writes JSON to `temp_editNeeded.txt`.
    *   **Step 2:** Calls `agentExecute` three times for different reviewers (`Agent_itemEditing_ReviewerX.py`).
    *   Writes reviewer suggestions to `temp_commentNeeded.txt` and appends them to `<ItemID>.txt`.
    *   **Step 3:** Calls `agentExecute` for `Agent_itemRevising.py`, passing the comments. Parses JSON result (`second_writing`).
    *   Updates `itemDataBase` (increments version, adds comments). Appends output to `<ItemID>.txt`.
    *   **Step 4:** Writes history to `temp_postrevisionNeeded.txt`. Calls `agentExecute` for `Agent_itemReediting.py`. Parses JSON result (`second_review`).
    *   **Conditional Branch:**
        *   **If `second_review$is_final` is TRUE:** Marks item as final in `itemDataBase`. Sets `final_writing = second_writing`. Appends final status to `<ItemID>.txt`.
        *   **If `second_review$is_final` is FALSE:** Appends Step 4 comments/suggestions to `<ItemID>.txt`. Writes relevant history to `temp_editorialVersionNeeded.txt`.
        *   **Step 5:** Calls `agentExecute` for `Agent_itemFinalwriting.py`. Parses JSON result (`thrid_revision`). Sets `final_writing = thrid_revision`. Appends output to `<ItemID>.txt`. Updates `itemDataBase`, marks as final, increments version.
    *   **Image Generation (Conditional):** If `final_writing$image_needed` is TRUE:
        *   Calls `Agent_image_gen.py` (local or API via `py$...` functions).
        *   Calls `Agent_image_verification.py` (`py$verify_medical_image`).
        *   Parses verification JSON and prints summary status.
    *   **Reporting:** Calls `py$convert_to_html` for each step's raw output. Assembles a final HTML document, embedding the image and verification results if available. Writes to `Item_Development_for_<ItemID>.html` and opens it.
    *   **Cleanup:** Calls `delete_temp_files()` to remove `temp_*.txt` files.
*   `poolAnalysisPrompt(x)`: Converts an R object `x` (like the `assignment` row) to a JSON string.
*   `delete_temp_files(directory)`: Deletes files starting with "temp" in the specified directory.

### `poolAnalysis.R`

*   *(Main Script Body)*: Analyzes the item bank.
    *   Loads packages, sources `global.R`. Sets environment.
    *   Reads `item_bank.json`.
    *   Calls `itembankCheck` to perform the analysis. Extracts `areasNeedItems`.
    *   Calls `agentExecute` to run `Agent_poolAnalysis.py` using `currentItembankInfo.json` as input.
    *   Calls `report_poolAnalysis` to generate the report.
*   `check_bounds(item_bank, df, dimension)`: Calculates proportions of `dimension` (e.g., 'disciplines') in `item_bank`, compares them to lower/upper bounds in `df`, and returns a data frame indicating if each category is 'lower', 'higher', or 'within' bounds, along with the actual proportion.
*   `itembankCheck(item_bank)`:
    *   Defines USMLE specifications as JSON strings.
    *   Parses JSON specs into data frames.
    *   Calls `check_bounds` for disciplines, systems, and competencies.
    *   Determines `areasNeedItems` by finding combinations of dimensions that are all 'lower'.
    *   Formats the results into a list (`itembankCheck_res`).
    *   Writes the detailed status check to `currentItembankInfo.json`.
    *   Returns the results list.
*   `report_poolAnalysis(Agent_poolAnalysis, areasNeedItems, output_format)`:
    *   Takes the LLM response (`Agent_poolAnalysis`) and `areasNeedItems`.
    *   Creates a temporary RMarkdown file (`.Rmd`).
    *   Populates the `.Rmd` file with a title, date, the LLM's narrative summary, and a markdown table generated from `areasNeedItems`.
    *   Uses `rmarkdown::render` to create an HTML or PDF report. Includes fallback to HTML if PDF fails.
    *   Prints results to the console.
    *   Returns the path to the generated report file.

### `Agent_*.py` Files (General Structure)

*   Typically read fixed prompt content from a corresponding `.txt` file (e.g., `edit_standard.txt`).
*   Read specific input data from a temporary `.txt` file (e.g., `temp_editNeeded.txt`).
*   Read language setting from `language_setting.txt`.
*   Combine prompts and inputs.
*   Initialize the `openai.OpenAI` client with API key, base URL, and model (placeholders like `"xxx"` are replaced by `replace_api_info` in R).
*   Call `client.chat.completions.create` with the system and user messages.
*   Print the `response.choices[0].message.content` to standard output, which is captured by `reticulate` in R.

### `Agent_image_gen.py`

*   `encode_image(image_path)`: (Defined within `verify_medical_image` but conceptually belongs here too) Reads an image file and returns its base64 encoded string.
*   `generate_medical_image(image_needed, image_type, image_prompt, pretrained_model_path, output_dir)`:
    *   Checks `image_needed`.
    *   Selects the appropriate fine-tuned UNet path based on `image_type`.
    *   Loads the UNet and Stable Diffusion pipeline from `pretrained_model_path`.
    *   Sets device (CPU/GPU).
    *   Generates an image using `pipe(prompt=...)`.
    *   Saves the image to `output_dir` with a timestamped filename.
    *   Returns the full path of the generated image.
*   `generate_medical_image_with_api(image_needed, image_type, prompt, output_dir)`:
    *   Checks `image_needed`.
    *   Initializes Google GenAI client (API key hardcoded - **SECURITY RISK - Should be configured externally**).
    *   Calls `client.models.generate_content` with the prompt.
    *   Extracts image data from the response.
    *   Saves the image to `output_dir`.
    *   Returns the image path.

### `Agent_image_verification.py`

*   `verify_medical_image(image_path, question_prompt, config_file_path, max_retries)`:
    *   Defines `encode_image` locally.
    *   Determines the working directory (`work_dir`).
    *   Reads `ModelInfo.csv`, filters for `type == 'image'`.
    *   Selects an image model's API key, base URL, and model name.
    *   *(The code attempts to update the config file itself, which is unusual and potentially problematic; typically, the client would just be initialized with the read credentials)*.
    *   Initializes `OpenAI` client with the selected image model credentials.
    *   Encodes the input `image_path` to base64.
    *   Reads `language_setting.txt`.
    *   **First Round (Answering):**
        *   Sends the image and question/options to the multimodal LLM using `system_prompt_answer`.
        *   Includes retry logic (`max_retries`) and attempts to parse JSON from the response (handling potential non-JSON text).
    *   **Second Round (Consistency):**
        *   Sends the image and `image_prompt` from `question_prompt` to the LLM using `system_prompt_consistency`.
        *   Includes similar retry and JSON parsing logic.
    *   Compares the model's selected answer from the first round to the `correct_answer` in `question_prompt`.
    *   Combines results from both rounds into a final JSON structure.
    *   Returns the final verification result as a JSON string. Includes error handling and prints raw outputs if failures occur.

### `Converter.py`

*   `extract_content(raw_content)`: Parses raw LLM text. Uses regex to find and separate content within `<think> </think>` tags, JSON blocks (within ```json ... ``` or just `{...}`), and the remaining markdown content. Returns `thinking_process`, `json_obj`, `markdown_content`.
*   `process_json_in_markdown(md_content)`: Temporarily replaces ```json ... ``` blocks in markdown with markers to prevent the markdown parser from interfering with them.
*   `restore_json_blocks(html_content)`: Finds the JSON markers inserted by `process_json_in_markdown`, extracts the JSON content, formats it nicely (if valid JSON), escapes it for HTML safety, and wraps it in a `<div class="json-viewer">...</div>` structure with appropriate CSS classes for syntax highlighting (via JS). Replaces the markers in the HTML.
*   `generate_html(thinking_process, json_obj, markdown_content, unique_id)`:
    *   Takes the parsed components from `extract_content`.
    *   Processes markdown (thinking and main content) using `process_json_in_markdown`, converts to HTML using `markdown.markdown`, then restores JSON blocks using `restore_json_blocks`.
    *   Formats the `json_obj` into a string.
    *   Constructs the final HTML document string, including:
        *   CSS styles for layout, thinking process disclosure, JSON viewer, and markdown body.
        *   JavaScript for client-side JSON syntax highlighting.
        *   Conditional sections for thinking process (collapsible), main markdown content, and the primary JSON object. Uses `unique_id` for targeting the main JSON display.
    *   Returns the complete HTML string.
*   `convert_to_html(content, index)`: Top-level function. Calls `extract_content` and `generate_html` to convert raw LLM content into a full HTML page. Handles potential errors during conversion. Generates a unique ID based on `index` if provided.
*   `merge_html_content(contents)`: (Not explicitly used in the R scripts provided, but present) Merges multiple complete HTML documents (presumably generated by `convert_to_html`) into one, attempting to consolidate `<head>` content and avoiding duplicate styles/scripts.
*   `batch_convert_to_html(contents)`: (Not explicitly used) Applies `convert_to_html` to a list of content strings and then merges them using `merge_html_content`.

## License
This project is licensed under the terms of the MIT License. See the [LICENSE file](https://github.com/FantaStiC898/AgentItemDev/blob/main/LICENSE) for details.

## Support
If you encounter any issues or have questions, please feel free to contact us.

Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.
